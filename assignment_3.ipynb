{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwUIZ-YZxGJT"
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Al1GIQ0NxGJa"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "1. You have to use only this notebook for all your code.\n",
    "2. All the results and plots should be mentioned in this notebook.\n",
    "3. For final submission, submit this notebook along with the report ( usual 2-4 pages, latex typeset, which includes the challenges faces and details of additional steps, if any)\n",
    "4. Marking scheme\n",
    "    -  **60%**: Your code should be able to detect bounding boxes using resnet 18, correct data loading and preprocessing. Plot any 5 correct and 5 incorrect sample detections from the test set in this notebook for both the approached (1 layer and 2 layer detection), so total of 20 plots.\n",
    "    -  **20%**: Use two layers (multi-scale feature maps) to detect objects independently as in SSD (https://arxiv.org/abs/1512.02325).  In this method, 1st detection will be through the last layer of Resnet18 and the 2nd detection could be through any layer before the last layer. SSD uses lower resolution layers to detect larger scale objects. \n",
    "    -  **20%**: Implement Non-maximum suppression (NMS) (should not be imported from any library) on the candidate bounding boxes.\n",
    "    \n",
    "5. Report AP for each of the three class and mAP score for the complete test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP3PWJMxvy3W"
   },
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCRrx2YWv00Z"
   },
   "outputs": [],
   "source": [
    "#This code will run on Google Colab\n",
    "#Create a folder named CS783_A3 in google drive \n",
    "#Extract the given PASCAL VOC train and test zip files in this folder such that you have this sort of folder arrangement \n",
    "# My Drive/CS783_A3/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/<all internal folders>\n",
    "# My Drive/CS783_A3/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/<all internal folders>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YdrH5Euv61F"
   },
   "outputs": [],
   "source": [
    "#Google colab authentication\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUnd2lT_xGJf"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "# Import other modules if required\n",
    "# Can use other libraries as well\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "resnet_input = 224#size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tS7MEkzjxGJ1"
   },
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "learning_rate =  0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "YcFxGFrzxGKB"
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, i.e. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the three classes(aeroplane, bottle, chair). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be four. This is important for applying the sliding window method later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UkvhIatxGKF"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "classes=['chair','bottle','aeroplane'] \n",
    "allclass=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "          'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa',\n",
    "          'train','tvmonitor']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPSoXDi3LjdC"
   },
   "outputs": [],
   "source": [
    "#extracting chairs, bottles and aeroplanes\n",
    "key='train'\n",
    "path='/content/drive/My Drive/CS783_A3/resized_full_'+key\n",
    "os.mkdir(path)\n",
    "\n",
    "for title in classes:\n",
    "    os.mkdir(path+'/'+title)\n",
    "    \n",
    "annotations = glob.glob('/content/drive/My Drive/CS783_A3/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/*.xml')\n",
    "imagedict = glob.glob('/content/drive/My Drive/CS783_A3/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/*.jpg')\n",
    "\n",
    "\n",
    "#same for both\n",
    "k=0\n",
    "l=0\n",
    "m=0\n",
    "for j in range(len(imagedict)):\n",
    "    tree = ET.parse(annotations[j])\n",
    "    root = tree.getroot()\n",
    "    img = cv2.imread(imagedict[j])\n",
    "    for obj in root.iter('object'):\n",
    "        title=obj[0].text\n",
    "        if(title in classes):\n",
    "            for bndbox in obj.iter('bndbox'):\n",
    "                image=[]\n",
    "                for i in range(4):\n",
    "                    image.append(int(np.rint(float(bndbox[i].text))))\n",
    "            crop=img[image[1]:image[3],image[0]:image[2]]\n",
    "            crop=cv2.resize(crop,(224,224))\n",
    "            if(title=='chair'):\n",
    "                cv2.imwrite(path+title+'/'+str(k)+'.jpg',crop)\n",
    "                k+=1\n",
    "            elif(title=='bottle'):\n",
    "                cv2.imwrite(path+title+'/'+str(l)+'.jpg',crop)\n",
    "                l+=1\n",
    "            else:\n",
    "                cv2.imwrite(path+title+'/'+str(m)+'.jpg',crop)\n",
    "                m+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWpTdxyeLpth"
   },
   "outputs": [],
   "source": [
    "\n",
    "#for extracting background\n",
    "\n",
    "def distance(p0, p1):\n",
    "            return np.sqrt((p0[0] - p1[0])**2 + (p0[1] - p1[1])**2)\n",
    "\n",
    "      \n",
    "annotations = glob.glob('/content/drive/My Drive/CS783_A3/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/*.xml')\n",
    "imagedict = glob.glob('/content/drive/My Drive/CS783_A3/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/*.jpg')\n",
    "\n",
    "final=[]\n",
    "k=0\n",
    "\n",
    "for j in range(len(imagedict)):\n",
    "    img = cv2.imread(imagedict[j])#take image\n",
    "    tree = ET.parse(annotations[j])#xml parse\n",
    "    root = tree.getroot()#get root\n",
    "    \n",
    "    \n",
    "    image=[]#this will stor bounding boxes for single object present in an image\n",
    "    bb=[]#this contains list of bounding boxes for all objects in an image\n",
    "    back=[]#this contains list of all background bounding boxes extracted from an image\n",
    "           #its length is 0 if no objects present(entire picture is background), 8 if single object is present, 12 otherwise\n",
    "                                                                                        \n",
    "    \n",
    "    for obj in root.iter('object'):\n",
    "        title=obj[0].text\n",
    "        if(title in allclass):\n",
    "            for bndbox in obj.iter('bndbox'):\n",
    "                image=[]\n",
    "                for i in range(4):\n",
    "                    image.append(int(np.rint(float(bndbox[i].text))))\n",
    "                bb.append(image)\n",
    "            \n",
    "    if(len(bb)==0):#no object present, then whole image is background\n",
    "        back.append(img)\n",
    "        backsort = back.copy()\n",
    "    \n",
    "    else:#if one or more objects present, extract 8 backgrounds, 2 from each corner (one horizontal, one vertical)\n",
    "        k=0\n",
    "        xmin=[]\n",
    "        xmax=[]\n",
    "        ymin=[]\n",
    "        ymax=[]\n",
    "        for i in range(len(bb)):\n",
    "            xmin.append(bb[i][0])\n",
    "            xmax.append(bb[i][2])\n",
    "            ymin.append(bb[i][1])\n",
    "            ymax.append(bb[i][3])\n",
    "            k+=1\n",
    "        \n",
    "        c1=[(a,b) for a,b in zip(xmin,ymin)]#top left corner\n",
    "        c2=[(a,b) for a,b in zip(xmax,ymin)]#top right corner\n",
    "        c3=[(a,b) for a,b in zip(xmin,ymax)]#bottom left corner\n",
    "        c4=[(a,b) for a,b in zip(xmax,ymax)]#bottom right corner\n",
    "        \n",
    "        #C1\n",
    "        l=0\n",
    "        u=0\n",
    "        dist=[]\n",
    "        for p in c1:\n",
    "            dist.append(distance((0,0),p))\n",
    "        point = c1[dist.index(min(dist))]#closest top-left to (0,0)\n",
    "        r1=point[0]\n",
    "        if(r1==min(xmin)):\n",
    "            d1=375\n",
    "        else:\n",
    "            d1=min([x[1] for x in bb if x[0]<r1])\n",
    "        d2=point[1]\n",
    "        if(d2==min(ymin)):\n",
    "            r2=500\n",
    "        else:\n",
    "            r2=min([x[0] for x in bb if x[1]<d2])\n",
    "        back.append(img[u:d1,l:r1])\n",
    "        back.append(img[u:d2,l:r2])\n",
    "            \n",
    "        #C2\n",
    "        r=500\n",
    "        u=0\n",
    "        dist=[]\n",
    "        for p in c2:\n",
    "            dist.append(distance((500,0),p))\n",
    "        point = c2[dist.index(min(dist))]#closest top-right to (500,0)\n",
    "        l1=point[0]\n",
    "        if(l1==max(xmax)):\n",
    "            d1=375\n",
    "        else:\n",
    "            d1=min([x[1] for x in bb if x[2]>l1])\n",
    "        d2=point[1]\n",
    "        if(d2==min(ymin)):\n",
    "            l2=0\n",
    "        else:\n",
    "            l2=max([x[2] for x in bb if x[1]<d2])\n",
    "        back.append(img[u:d1,l1:r])\n",
    "        back.append(img[u:d2,l2:r])\n",
    "        \n",
    "        #C3\n",
    "        l=0\n",
    "        d=375\n",
    "        dist=[]\n",
    "        for p in c3:\n",
    "            dist.append(distance((0,375),p))#closest bottom-left to (0,375)\n",
    "        point = c3[dist.index(min(dist))]\n",
    "        r1=point[0]\n",
    "        if(r1==min(xmin)):\n",
    "            u1=0\n",
    "        else:\n",
    "            u1=max([x[3] for x in bb if x[0]<r1])\n",
    "        u2=point[1]\n",
    "        if(u2==max(ymax)):\n",
    "            r2=500\n",
    "        else:\n",
    "            r2=min([x[0] for x in bb if x[3]>u2])\n",
    "        back.append(img[u1:d,l:r1])\n",
    "        back.append(img[u2:d,l:r2])\n",
    "        \n",
    "        #C4\n",
    "        r=500\n",
    "        d=375\n",
    "        dist=[]\n",
    "        for p in c4:\n",
    "            dist.append(distance((500,375),p))\n",
    "        point = c4[dist.index(min(dist))]#closest bottom right to (500,375)\n",
    "        l1=point[0]\n",
    "        if(l1==max(xmax)):\n",
    "            u1=0\n",
    "        else:\n",
    "            u1=max([x[3] for x in bb if x[2]>l1])\n",
    "        u2=point[1]\n",
    "        if(u2==max(ymax)):\n",
    "            l2=0\n",
    "        else:\n",
    "            l2=max([x[2] for x in bb if x[3]>u2])\n",
    "        back.append(img[u1:d,l1:r])\n",
    "        back.append(img[u2:d,l2:r])\n",
    "        \n",
    "        if(len(bb)>1):#if more than 1 object present, extract 4 more backgrounds from within detected objects\n",
    "            \n",
    "            leftright = min(xmax)\n",
    "            ilr = xmax.index(leftright)\n",
    "            rightleft = max(xmin)\n",
    "            irl = xmin.index(rightleft)\n",
    "            topbot = min(ymax)\n",
    "            itb = ymax.index(topbot)\n",
    "            bottop = max(ymin)\n",
    "            ibt = ymin.index(bottop)\n",
    "            \n",
    "            bb1=bb.copy()\n",
    "            bb2=bb.copy()\n",
    "            bb1.remove(bb[ilr])\n",
    "            if(ilr!=irl):\n",
    "                bb1.remove(bb[irl])\n",
    "            bb2.remove(bb[itb])\n",
    "            if(itb!=ibt):\n",
    "                bb2.remove(bb[ibt])\n",
    "            \n",
    "            if(len(bb1)==0):\n",
    "                toptop=0\n",
    "                botbot=375\n",
    "            else:\n",
    "                toptop=min([x[1] for x in bb1])\n",
    "                botbot=max([x[3] for x in bb1])\n",
    "            if(len(bb2)==0):\n",
    "                leftleft=0\n",
    "                rightright=500\n",
    "            else:\n",
    "                leftleft=min(x[0] for x in bb2)\n",
    "                rightright=max(x[2] for x in bb2)\n",
    "            \n",
    "            im1 = img[0:toptop,leftright:rightleft]\n",
    "            im2 = img[botbot:375,leftright:rightleft]\n",
    "            im3 = img[topbot:bottop,0:leftleft]\n",
    "            im4 = img[topbot:bottop,rightright:500]\n",
    "            \n",
    "            back.append(im1)\n",
    "            back.append(im2)\n",
    "            back.append(im3)\n",
    "            back.append(im4)\n",
    "        \n",
    "        val={}\n",
    "        backsort=[]\n",
    "        for i in range(len(back)):\n",
    "            val[i] = back[i].size#create dictionary based on size for backgrounds extracted from this image\n",
    "            \n",
    "        valsort = sorted(val.items(), key=operator.itemgetter(1),reverse=True)#sort in descending order to get maximum area backgrounds from that image\n",
    "        backsort=[back[x[0]] for x in valsort]\n",
    "        backsort = backsort[:len(bb)]#take those many backgrounds as ther are objects in that image\n",
    "\n",
    "    final.append(backsort)#stores a list of all backgrounds extract from entire training data\n",
    "\n",
    "\n",
    "\n",
    "full=[]    \n",
    "sizes=[]\n",
    "for lst in final:\n",
    "    sizes.append(len(lst))\n",
    "    for img in lst:\n",
    "        if(img.size!=0):\n",
    "            full.append(img)#store all backgrounds\n",
    "\n",
    "np.save('/content/drive/My Drive/CS783_A3/all_backgrounds.npy',full)#store for future use\n",
    "\n",
    "\n",
    "#sorting full acc. to size        \n",
    "val={}\n",
    "fullsort=[]\n",
    "for i in range(len(full)):\n",
    "    val[i] = full[i].size#create dictionary based on size for entire dataset\n",
    "    \n",
    "valsort = sorted(val.items(), key=operator.itemgetter(1),reverse=True)#sort in descending order to get maximum area backgrounds from entire dataset\n",
    "fullsort=[full[x[0]] for x in valsort]\n",
    "\n",
    "\n",
    "#for limited background samples, slice fullsort accordingly\n",
    "fullsort = fullsort[10000:]# we have taken last backgrounds as they conatin least amount of information, and truly serve as background\n",
    "\n",
    "#create background folder\n",
    "k=0\n",
    "path='/content/drive/My Drive/CS783_A3/resized_full_train/background'\n",
    "os.mkdir(path)\n",
    "\n",
    "for img in fullsort:\n",
    "    res=cv2.resize(img,(224,224))\n",
    "    cv2.imwrite(path+'/'+str(k)+'.jpg',res)#write images to folder\n",
    "    k+=1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ns35eHzZxGKj"
   },
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 4 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgIFP7_TSXo0"
   },
   "outputs": [],
   "source": [
    "train_dir = '/content/drive/My Drive/CS783_A3/resized_full_train'\n",
    "\n",
    "def load_train(datadir):\n",
    "    train_transforms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(), transforms.RandomHorizontalFlip()])\n",
    "    train_data = datasets.ImageFolder(datadir, transform=train_transforms)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    return trainloader\n",
    "\n",
    "trainloader = load_train(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JxF2UygxGKz"
   },
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ey72-jA8URnl"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ONE LAYER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9Z5cLNkUCh6"
   },
   "outputs": [],
   "source": [
    "#code for using CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV7NqnCmULhh"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HErcjIexGK2"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():#freezing all layers\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Sequential(nn.ReLU(),nn.Linear(num_ftrs, 4),nn.Softmax(dim=1))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzMdAG3bU5LG"
   },
   "outputs": [],
   "source": [
    "model.to(device)#for seeing Resnet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n16sDu56VAEp"
   },
   "outputs": [],
   "source": [
    "#for checking model summary and finding number of trainable parameters\n",
    "from torchsummary import summary\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Xm6pg-1WJCJ"
   },
   "source": [
    "SINGLE LAYER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6W9BlfA6xGLK"
   },
   "outputs": [],
   "source": [
    "#declaring traininfg parameters\n",
    "epochs = 5\n",
    "steps = 0#for counting steps\n",
    "running_loss = 0\n",
    "print_every = 10#to print loss after ever print_every steps\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcUP0GHdxGLV"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQwjgNToV--3"
   },
   "outputs": [],
   "source": [
    "#saving the model  \n",
    "torch.save(model, '/content/drive/My Drive/CS783_A3/single_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lexPhOvtjzCz"
   },
   "source": [
    "Model for second layer detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzV3D3BZxGLh"
   },
   "outputs": [],
   "source": [
    "#model obtained after removing last conv layers and keeping other pretrained layers\n",
    "\n",
    "class ResNet18_new(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ResNet18_new, self).__init__()\n",
    "        self.features = nn.Sequential(*list(model.children())[:-3])\n",
    "        self.adpavg2dpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc=nn.Sequential(nn.Linear(256,4),nn.Softmax(dim=1))\n",
    "        ct = 0 \n",
    "        for child in self.features.children():#freezing all the pretrained layer except the layers following outermost conv layer(including conv layer)\n",
    "            ct += 1\n",
    "            if ct < 7:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x= self.adpavg2dpool(x)\n",
    "        x = x.view(x.size(0), - 1)\n",
    "        x=self.fc(x)                                       \n",
    "        return x\n",
    "model = models.resnet18(pretrained=True)\n",
    "res18 = ResNet18_new(model)\n",
    "res18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VuVJJMHiLjn"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(res18.fc.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw5i7mtqiU02"
   },
   "outputs": [],
   "source": [
    "#declaring traininfg parameters\n",
    "\n",
    "epochs = 5\n",
    "steps = 0#for counting steps\n",
    "running_loss = 0\n",
    "print_every = 10#to print loss after ever print_every steps\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVwbcwcgj6n4"
   },
   "source": [
    "training of model for second layer detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DI3GLu2Wimz7"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = res18.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            #model.eval()\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \")\n",
    "            running_loss = 0\n",
    "            res18.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fUbUufoi0_c"
   },
   "outputs": [],
   "source": [
    "torch.save(res18, '/content/drive/My Drive/CS783_A3/second_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wELZCsoyxGL2"
   },
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_H7ZbDGqxGL8"
   },
   "outputs": [],
   "source": [
    "def sliding_window(size, stepSize , windowSize):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, size, stepSize):\n",
    "      for x in range(0,size, stepSize):\n",
    "        # yield the current window\n",
    "        if(x+windowSize[0]<=224 and y+windowSize[1]<=224):\n",
    "          yield ([x, y, x + windowSize[0], y + windowSize[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG8isWs_xGMH"
   },
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cozKp0IlxGML"
   },
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "\t# if there are no boxes, return an empty list\n",
    "\tif len(boxes) == 0:\n",
    "\t\treturn []\n",
    "\t# if the bounding boxes integers, convert them to floats --\n",
    "\t# this is important since we'll be doing a bunch of divisions\n",
    "\tif boxes.dtype.kind == \"i\":\n",
    "\t\tboxes = boxes.astype(\"float\")\n",
    "\t# initialize the list of picked indexes\t\n",
    "\tpick = []\n",
    "\t# grab the coordinates of the bounding boxes\n",
    "\tx1 = boxes[:,0]\n",
    "\ty1 = boxes[:,1]\n",
    "\tx2 = boxes[:,2]\n",
    "\ty2 = boxes[:,3]\n",
    "\t# compute the area of the bounding boxes and sort the bounding\n",
    "\t# boxes by the bottom-right y-coordinate of the bounding box\n",
    "\tarea = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\tidxs = np.argsort(y2)\n",
    "\t# keep looping while some indexes still remain in the indexes\n",
    "\t# list\n",
    "\twhile len(idxs) > 0:\n",
    "\t\t# grab the last index in the indexes list and add the\n",
    "\t\t# index value to the list of picked indexes\n",
    "\t\tlast = len(idxs) - 1\n",
    "\t\ti = idxs[last]\n",
    "\t\tpick.append(i)\n",
    "\t\t# find the largest (x, y) coordinates for the start of\n",
    "\t\t# the bounding box and the smallest (x, y) coordinates\n",
    "\t\t# for the end of the bounding box\n",
    "\t\txx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "\t\tyy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "\t\txx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "\t\tyy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\t\t# compute the width and height of the bounding box\n",
    "\t\tw = np.maximum(0, xx2 - xx1 + 1)\n",
    "\t\th = np.maximum(0, yy2 - yy1 + 1)\n",
    "\t\t# compute the ratio of overlap\n",
    "\t\toverlap = (w * h) / area[idxs[:last]]\n",
    "\t\t# delete all indexes from the index list that have\n",
    "\t\tidxs = np.delete(idxs, np.concatenate(([last],\n",
    "\t\t\tnp.where(overlap > overlapThresh)[0])))\n",
    "\t# return only the bounding boxes that were picked using the\n",
    "\t# integer data type\n",
    "\treturn boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0eQ9U8qkBq7"
   },
   "source": [
    "Single layer detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u1imxiRxGMW"
   },
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VGv-1NVWsva"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-fLVr3QxOAN"
   },
   "source": [
    "ONE LAYER DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCBjdjifxGMZ"
   },
   "outputs": [],
   "source": [
    "thresh=0.5\n",
    "path1='/content/drive/My Drive/CS783_A3/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'\n",
    "path2='/content/drive/My Drive/CS783_A3/outputs'\n",
    "os.mkdir(path2)\n",
    "images=glob.glob(path1+'/*.jpg')\n",
    "k=0\n",
    "for im in images:\n",
    "    name=im[-10:-4]\n",
    "    img=cv2.imread(im)\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    for window_size in [(64,64),(64,128),(128,64),(24,48),(24,24),(48,24),(104,104),(104,208),(208,104)]:\n",
    "        for window in sliding_window(224, 20, window_size):\n",
    "            crop=img[window[1]:window[3],window[0]:window[2]]\n",
    "            res = cv2.resize(crop,(224,224))\n",
    "            t=transforms.ToTensor()(res)\n",
    "            t=t.unsqueeze_(0)\n",
    "            t=t.to(device)\n",
    "            out=model(t)\n",
    "            cpu_pred = out.cpu()\n",
    "            result = cpu_pred.data.numpy()\n",
    "            m=max(out[0].tolist())\n",
    "            if(m>=thresh):\n",
    "              window.append(result.argmax())\n",
    "              if(k==0):\n",
    "                boxes = np.array([window])\n",
    "              else:\n",
    "                boxes=np.append(boxes,np.array([window]),axis=0)\n",
    "            k+=1\n",
    "    output_nms=non_max_suppression_fast(boxes, 0.3)\n",
    "    #following code saves bounding boxes and their labels for each image in a text file,\n",
    "    with open(path2+'/'+name+'.txt','w') as f:\n",
    "        for a in output_nms:\n",
    "            f.write(classes[int(a[-1])]+' '+str(a[0])+' '+str(a[1])+' '+str(a[3])+' '+str(a[4])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvYaugNvbxy2"
   },
   "source": [
    "TWO LAYER DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huKkmEK_xGMo"
   },
   "outputs": [],
   "source": [
    "#defining custom architecture\n",
    "class ResNet18_new(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ResNet18_new, self).__init__()\n",
    "        self.features = nn.Sequential(*list(model.children())[:-3])\n",
    "        self.adpavg2dpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc=nn.Linear(256,4)\n",
    "        ct = 0 \n",
    "        for child in self.features.children():\n",
    "            ct += 1\n",
    "            if ct < 7:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x= self.adpavg2dpool(x)\n",
    "        x = x.view(x.size(0), - 1)\n",
    "        x=self.fc(x)                                       \n",
    "        return x\n",
    "\n",
    "model2 = models.resnet18(pretrained=True)\n",
    "res18 = ResNet18_new(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pED1yEimqzMr"
   },
   "outputs": [],
   "source": [
    "thresh=0.5\n",
    "path1='/content/drive/My Drive/CS783_A3/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'\n",
    "path2='/content/drive/My Drive/CS783_A3/outputs'\n",
    "os.mkdir(path2)\n",
    "images=glob.glob(path1+'/*.jpg')\n",
    "k=0\n",
    "for im in images:\n",
    "    name=im[-10:-4]\n",
    "    img=cv2.imread(im)\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    for window_size in [(64,64),(64,128),(128,64),(24,48),(24,24),(48,24),(104,104),(104,208),(208,104)]:\n",
    "        for window in sliding_window(224, 20, window_size):\n",
    "            crop=img[window[1]:window[3],window[0]:window[2]]\n",
    "            res = cv2.resize(crop,(224,224))\n",
    "            t=transforms.ToTensor()(res)\n",
    "            t=t.unsqueeze_(0)\n",
    "            t=t.to(device)\n",
    "            out=model(t)\n",
    "            cpu_pred = out.cpu()\n",
    "            result = cpu_pred.data.numpy()\n",
    "            m=max(out[0].tolist())\n",
    "            if(m>=thresh):\n",
    "              window.append(result.argmax())\n",
    "              if(k==0):\n",
    "                boxes = np.array([window])\n",
    "              else:\n",
    "                boxes=np.append(boxes,np.array([window]),axis=0)\n",
    "            k+=1\n",
    "    output_nms=non_max_suppression_fast(boxes, 0.3)\n",
    "    #following code saves bounding boxes and their labels for each image in a text file,\n",
    "    with open(path2+'/'+name+'.txt','w') as f:\n",
    "        for a in output_nms:\n",
    "            f.write(classes[int(a[-1])]+' '+str(a[0])+' '+str(a[1])+' '+str(a[3])+' '+str(a[4])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
